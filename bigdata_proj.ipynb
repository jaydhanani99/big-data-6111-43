{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "9075a8e0-75cf-428f-8eac-9218daabf47c",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "# Weather forecasting with PySpark\n",
        "## Big Data Computing final project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "35f6270a-ad0e-4157-8f91-75f72d42023f",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "###Define some global constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c0e76bca-68f2-46ed-a5f1-3bf27bd0a9fe",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "DATASET_PATH: str = 'dbfs:/bigdata_proj/datasets/historical-hourly-weather/'\n",
        "MODELS_PATH: str = 'dbfs:/bigdata_proj/models/historical-hourly-weather/'\n",
        "\n",
        "RANDOM_SEED: int = 42\n",
        "  \n",
        "SLOW_OPERATIONS: bool = True\n",
        "  \n",
        "# True to save the computation of datataset preprocessing, fitted pipelines and trained models to the filesystem\n",
        "SAVE_COMPUTATIONS: bool = True\n",
        "  \n",
        "# True to load the sampled dataset from the filesystem, False to compute it from the raw one\n",
        "LOAD_SAMPLED_DATASET: bool = True\n",
        "SAMPLED_DATASET_PATH: str = f'{DATASET_PATH}aggregated_sampled_weather_measurements.csv'\n",
        "  \n",
        "# True to load the encoding pipeline from the filesystem, False to compute it from scratch\n",
        "LOAD_ECONDING_PIPELINE: bool = True\n",
        "ENCODING_PIPELINE_PATH: str = f'{MODELS_PATH}data_encoder'\n",
        "\n",
        "# True to load pretrained models from the filesystem, False to compute them from scratch\n",
        "LOAD_PRETRAINED_MODELS: bool = True\n",
        "RANDOM_FOREST_MODEL_PATH: str = f'{MODELS_PATH}rnd_forest'\n",
        "RANDOM_FOREST_CROSS_VALIDATION_MODEL_PATH: str = f'{MODELS_PATH}rnd_forest_cv'\n",
        "LOGISTIC_REGRESSION_CROSS_VALIDATION_MODEL_PATH: str = f'{MODELS_PATH}log_reg_cv'\n",
        "\n",
        "# necessary due to DataBricks community edition limits (training on a dataframe larger than this threshold causes an Internal Server Error)\n",
        "MAX_TRAIN_SIZE: int = 999_999"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "e2a94ed4-2ff7-4e88-8a9d-73e9feac8f4d",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "###Import PySpark packages and other dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "a381d659-aa64-40d8-896f-cb3ff18076fa",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.pipeline import PipelineModel\n",
        "from pyspark.ml.tuning import CrossValidatorModel\n",
        "\n",
        "from typing import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "14ff1bf9-6372-4d66-b7f0-bc0d52f5496d",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "## Dataset initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "914d8406-ec68-4349-b3dd-b62440e65aa3",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Download the dataset\n",
        "Original source: [kaggle.com/selfishgene/historical-hourly-weather-data](https://www.kaggle.com/selfishgene/historical-hourly-weather-data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c99e1ddc-8df8-4c33-9f0f-76bcde7ac0f9",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-19 19:56:06 URL:https://raw.githubusercontent.com/andrea-gasparini/big-data-weather-forecasting/master/dataset/historical-hourly-weather-dataset.zip [12655281/12655281] -> \"/tmp/dataset.zip\" [1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /tmp/dataset.zip\n",
            "   creating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/\n",
            "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/._committed_7616641238230246128.crc  \n",
            "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/.part-00000-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5515-1-c000.csv.crc  \n",
            "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/.part-00006-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5521-1-c000.csv.crc  \n",
            "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/.part-00002-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5517-1-c000.csv.crc  \n",
            "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/.part-00004-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5519-1-c000.csv.crc  \n",
            "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/part-00003-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5518-1-c000.csv  \n",
            "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/_committed_7616641238230246128  \n",
            "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/_committed_vacuum6920010656788597249  \n",
            "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/part-00005-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5520-1-c000.csv  \n",
            "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/part-00002-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5517-1-c000.csv  \n",
            "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/_started_5785058191842647654  \n",
            "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/part-00001-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5516-1-c000.csv  \n",
            "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/_committed_777980178201020908  \n",
            "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/_committed_5785058191842647654  \n",
            "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/._committed_5785058191842647654.crc  \n",
            "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/.part-00001-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5516-1-c000.csv.crc  \n",
            "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/.part-00003-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5518-1-c000.csv.crc  \n",
            "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/part-00007-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5522-1-c000.csv  \n",
            "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/part-00006-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5521-1-c000.csv  \n",
            "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/._started_5785058191842647654.crc  \n",
            "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/._committed_777980178201020908.crc  \n",
            "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/part-00000-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5515-1-c000.csv  \n",
            "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/.part-00007-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5522-1-c000.csv.crc  \n",
            "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/.part-00005-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5520-1-c000.csv.crc  \n",
            "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/part-00004-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5519-1-c000.csv  \n",
            "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/._committed_vacuum6920010656788597249.crc  \n",
            "  inflating: /tmp/dataset/city_attributes.csv  \n",
            "  inflating: /tmp/dataset/humidity.csv  \n",
            "  inflating: /tmp/dataset/pressure.csv  \n",
            "  inflating: /tmp/dataset/temperature.csv  \n",
            "  inflating: /tmp/dataset/weather_description.csv  \n",
            "  inflating: /tmp/dataset/wind_direction.csv  \n",
            "  inflating: /tmp/dataset/wind_speed.csv  \n"
          ]
        }
      ],
      "source": [
        "%sh\n",
        "wget --no-verbose https://github.com/andrea-gasparini/big-data-weather-forecasting/raw/master/dataset/historical-hourly-weather-dataset.zip -O /tmp/dataset.zip\n",
        "unzip -u /tmp/dataset.zip -d /tmp/dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "be06d137-7912-43f2-9b0d-43d2ef991031",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "###Move the dataset from Databricks local driver node's file system to DBFS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "64eff477-1070-4fc7-91e5-15235f5dc367",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "for file in dbutils.fs.ls('file:/tmp/dataset'):\n",
        "    dbutils.fs.mv(file.path, f'{DATASET_PATH}{file.name}', recurse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1098907a-156f-436c-b329-75301620a64f",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .table-result-container {\n",
              "    max-height: 300px;\n",
              "    overflow: auto;\n",
              "  }\n",
              "  table, th, td {\n",
              "    border: 1px solid black;\n",
              "    border-collapse: collapse;\n",
              "  }\n",
              "  th, td {\n",
              "    padding: 5px;\n",
              "  }\n",
              "  th {\n",
              "    text-align: left;\n",
              "  }\n",
              "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/bigdata_proj/datasets/historical-hourly-weather/aggregated_sampled_weather_measurements.csv/</td><td>aggregated_sampled_weather_measurements.csv/</td><td>0</td><td>1710607244000</td></tr><tr><td>dbfs:/bigdata_proj/datasets/historical-hourly-weather/city_attributes.csv</td><td>city_attributes.csv</td><td>1614</td><td>1710878212000</td></tr><tr><td>dbfs:/bigdata_proj/datasets/historical-hourly-weather/humidity.csv</td><td>humidity.csv</td><td>9075077</td><td>1710878209000</td></tr><tr><td>dbfs:/bigdata_proj/datasets/historical-hourly-weather/pressure.csv</td><td>pressure.csv</td><td>12155911</td><td>1710878209000</td></tr><tr><td>dbfs:/bigdata_proj/datasets/historical-hourly-weather/temperature.csv</td><td>temperature.csv</td><td>13971171</td><td>1710878210000</td></tr><tr><td>dbfs:/bigdata_proj/datasets/historical-hourly-weather/weather_description.csv</td><td>weather_description.csv</td><td>21858089</td><td>1710878208000</td></tr><tr><td>dbfs:/bigdata_proj/datasets/historical-hourly-weather/wind_direction.csv</td><td>wind_direction.csv</td><td>10171003</td><td>1710878208000</td></tr><tr><td>dbfs:/bigdata_proj/datasets/historical-hourly-weather/wind_speed.csv</td><td>wind_speed.csv</td><td>7457531</td><td>1710878212000</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "addedWidgets": {},
              "aggData": [],
              "aggError": "",
              "aggOverflow": false,
              "aggSchema": [],
              "aggSeriesLimitReached": false,
              "aggType": "",
              "arguments": {},
              "columnCustomDisplayInfos": {},
              "data": [
                [
                  "dbfs:/bigdata_proj/datasets/historical-hourly-weather/aggregated_sampled_weather_measurements.csv/",
                  "aggregated_sampled_weather_measurements.csv/",
                  0,
                  1710607244000
                ],
                [
                  "dbfs:/bigdata_proj/datasets/historical-hourly-weather/city_attributes.csv",
                  "city_attributes.csv",
                  1614,
                  1710878212000
                ],
                [
                  "dbfs:/bigdata_proj/datasets/historical-hourly-weather/humidity.csv",
                  "humidity.csv",
                  9075077,
                  1710878209000
                ],
                [
                  "dbfs:/bigdata_proj/datasets/historical-hourly-weather/pressure.csv",
                  "pressure.csv",
                  12155911,
                  1710878209000
                ],
                [
                  "dbfs:/bigdata_proj/datasets/historical-hourly-weather/temperature.csv",
                  "temperature.csv",
                  13971171,
                  1710878210000
                ],
                [
                  "dbfs:/bigdata_proj/datasets/historical-hourly-weather/weather_description.csv",
                  "weather_description.csv",
                  21858089,
                  1710878208000
                ],
                [
                  "dbfs:/bigdata_proj/datasets/historical-hourly-weather/wind_direction.csv",
                  "wind_direction.csv",
                  10171003,
                  1710878208000
                ],
                [
                  "dbfs:/bigdata_proj/datasets/historical-hourly-weather/wind_speed.csv",
                  "wind_speed.csv",
                  7457531,
                  1710878212000
                ]
              ],
              "datasetInfos": [],
              "dbfsResultPath": null,
              "isJsonSchema": true,
              "metadata": {
                "isDbfsCommandResult": false
              },
              "overflow": false,
              "plotOptions": {
                "customPlotOptions": {},
                "displayType": "table",
                "pivotAggregation": null,
                "pivotColumns": null,
                "xColumns": null,
                "yColumns": null
              },
              "removedWidgets": [],
              "schema": [
                {
                  "metadata": "{}",
                  "name": "path",
                  "type": "\"string\""
                },
                {
                  "metadata": "{}",
                  "name": "name",
                  "type": "\"string\""
                },
                {
                  "metadata": "{}",
                  "name": "size",
                  "type": "\"long\""
                },
                {
                  "metadata": "{}",
                  "name": "modificationTime",
                  "type": "\"long\""
                }
              ],
              "type": "table"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "%fs ls /bigdata_proj/datasets/historical-hourly-weather"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "2aa03979-966a-4324-a401-bf32252e6856",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "###Load dataset into Spark DataFrame objects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "1abc00d8-c34f-4b23-a8a4-ca12837daef1",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Dataset shape and schema\n",
        "The raw dataset downloaded from kaggle is composed of 7 different `.csv` files:\n",
        "- `city_attributes.csv` contains geographical information about the different cities for which there are weather measurements\n",
        "- `weather_description.csv` contains the textual description of the weather conditions, where each column refers to a different city and each row refers to a specific `datetime` in which the weather condition occurred\n",
        "- Each one of the other 5 csv follows the same structure as `weather_description.csv` and contains the measurements of the following metrics: `humidity`,  `pressure`, `temperature`, `wind_direction`, `wind_speed`\n",
        "\n",
        "Except for `city_attributes.csv`, all the other files contains about **45.000** records of hourly weather measurements, that multiplied by the **36** cities results in approximately **1.500.000** records."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4fb642d4-5361-4fb1-b406-ce3e3431bbd5",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# rest of your code\n",
        "weather_conditions_df = spark.read.csv(f'{DATASET_PATH}weather_description.csv', header=True, inferSchema=True)\n",
        "humidity_df = spark.read.csv(f'{DATASET_PATH}humidity.csv', header=True, inferSchema=True)\n",
        "pressure_df = spark.read.csv(f'{DATASET_PATH}pressure.csv', header=True, inferSchema=True)\n",
        "temperature_df = spark.read.csv(f'{DATASET_PATH}temperature.csv', header=True, inferSchema=True)\n",
        "city_attributes_df = spark.read.csv(f'{DATASET_PATH}city_attributes.csv', header=True, inferSchema=True)\n",
        "wind_direction_df = spark.read.csv(f'{DATASET_PATH}wind_direction.csv', header=True, inferSchema=True)\n",
        "wind_speed_df = spark.read.csv(f'{DATASET_PATH}wind_speed.csv', header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Trying to print the shape and see whats going on**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_dataframe_shape(dataframe: DataFrame) -> None:\n",
        "    rows_count = dataframe.count()\n",
        "    columns_count = len(dataframe.columns)\n",
        "    print(f'The shape of the dataset is {rows_count} rows by {columns_count} columns', end='\\n\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Using the function above for city_attributes.csv, Additonally pringing the shema as well**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The shape of the dataset is 36 rows by 4 columns\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_dataframe_shape(city_attributes_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- City: string (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            " |-- Latitude: double (nullable = true)\n",
            " |-- Longitude: double (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "city_attributes_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Row(City='Vancouver', Country='Canada', Latitude=49.24966, Longitude=-123.119339),\n",
              " Row(City='Portland', Country='United States', Latitude=45.523449, Longitude=-122.676208),\n",
              " Row(City='San Francisco', Country='United States', Latitude=37.774929, Longitude=-122.419418),\n",
              " Row(City='Seattle', Country='United States', Latitude=47.606209, Longitude=-122.332069),\n",
              " Row(City='Los Angeles', Country='United States', Latitude=34.052231, Longitude=-118.243683)]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "city_attributes_df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Using the function print_data_frame we made above for weather_description.csv, Additonally pringing the shema as well.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The shape of the dataset is 45253 rows by 37 columns\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_dataframe_shape(weather_conditions_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- datetime: timestamp (nullable = true)\n",
            " |-- Vancouver: string (nullable = true)\n",
            " |-- Portland: string (nullable = true)\n",
            " |-- San Francisco: string (nullable = true)\n",
            " |-- Seattle: string (nullable = true)\n",
            " |-- Los Angeles: string (nullable = true)\n",
            " |-- San Diego: string (nullable = true)\n",
            " |-- Las Vegas: string (nullable = true)\n",
            " |-- Phoenix: string (nullable = true)\n",
            " |-- Albuquerque: string (nullable = true)\n",
            " |-- Denver: string (nullable = true)\n",
            " |-- San Antonio: string (nullable = true)\n",
            " |-- Dallas: string (nullable = true)\n",
            " |-- Houston: string (nullable = true)\n",
            " |-- Kansas City: string (nullable = true)\n",
            " |-- Minneapolis: string (nullable = true)\n",
            " |-- Saint Louis: string (nullable = true)\n",
            " |-- Chicago: string (nullable = true)\n",
            " |-- Nashville: string (nullable = true)\n",
            " |-- Indianapolis: string (nullable = true)\n",
            " |-- Atlanta: string (nullable = true)\n",
            " |-- Detroit: string (nullable = true)\n",
            " |-- Jacksonville: string (nullable = true)\n",
            " |-- Charlotte: string (nullable = true)\n",
            " |-- Miami: string (nullable = true)\n",
            " |-- Pittsburgh: string (nullable = true)\n",
            " |-- Toronto: string (nullable = true)\n",
            " |-- Philadelphia: string (nullable = true)\n",
            " |-- New York: string (nullable = true)\n",
            " |-- Montreal: string (nullable = true)\n",
            " |-- Boston: string (nullable = true)\n",
            " |-- Beersheba: string (nullable = true)\n",
            " |-- Tel Aviv District: string (nullable = true)\n",
            " |-- Eilat: string (nullable = true)\n",
            " |-- Haifa: string (nullable = true)\n",
            " |-- Nahariyya: string (nullable = true)\n",
            " |-- Jerusalem: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "weather_conditions_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Row(datetime=datetime.datetime(2012, 10, 1, 12, 0), Vancouver=None, Portland=None, San Francisco=None, Seattle=None),\n",
              " Row(datetime=datetime.datetime(2012, 10, 1, 13, 0), Vancouver='mist', Portland='scattered clouds', San Francisco='light rain', Seattle='sky is clear'),\n",
              " Row(datetime=datetime.datetime(2012, 10, 1, 14, 0), Vancouver='broken clouds', Portland='scattered clouds', San Francisco='sky is clear', Seattle='sky is clear'),\n",
              " Row(datetime=datetime.datetime(2012, 10, 1, 15, 0), Vancouver='broken clouds', Portland='scattered clouds', San Francisco='sky is clear', Seattle='sky is clear'),\n",
              " Row(datetime=datetime.datetime(2012, 10, 1, 16, 0), Vancouver='broken clouds', Portland='scattered clouds', San Francisco='sky is clear', Seattle='sky is clear')]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "weather_conditions_df[weather_conditions_df.columns[:5]].head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset preprocessing Starts\n",
        "\n",
        "### For Columns\n",
        "Setting some alias for colums to standardize throughout the code further"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATETIME_COL = 'datetime'\n",
        "HUMIDITY_COL = 'humidity'\n",
        "PRESSURE_COL = 'pressure'\n",
        "TEMPERATURE_COL = 'temperature'\n",
        "WIND_DIRECTION_COL = 'wind_direction'\n",
        "WIND_SPEED_COL = 'wind_speed'\n",
        "LATITUDE_COL = 'latitude'\n",
        "LONGITUDE_COL = 'longitude'\n",
        "CITY_COL = 'city'\n",
        "COUNTRY_COL = 'country'\n",
        "WEATHER_CONDITION_COL = 'weather_condition'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Create a single DataFrame that includes all data from the others**\n",
        "\n",
        "Reason to do this : The as we have seen from the shapes above, the dataset needs to be made suitable for Machine Learning purposes. The bese solution here is to use a single DataFrame object which includes all the information. Here we can have one column each for each metric, maybe some columns for the information such as city, geographical positioning etc and lastely one column for weather condition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "def filter_dataframe_by_city_column(dataframe: DataFrame,\n",
        "                                    city_name: str,\n",
        "                                    new_column_name: str) -> DataFrame:\n",
        "    '''\n",
        "    Args:\n",
        "        - dataframe: a `DataFrame` with a datetime column and n cities columns,\n",
        "                     where the records are the related hourly measurements\n",
        "        - city_name: city name between the ones in the dataframe\n",
        "        - new_column_name: name to replace the city name\n",
        "        \n",
        "    Returns: \n",
        "        a new `DataFrame` with:\n",
        "            - the datetime column\n",
        "            - a single column of measurements related to the `city_name`\n",
        "              and renamed as `new_column_name`\n",
        "    '''\n",
        "    return dataframe.withColumn(new_column_name, col(city_name)) \\\n",
        "                    .select([DATETIME_COL, new_column_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def join_dataframes(dataframes: List[DataFrame], column_name: str) -> DataFrame:\n",
        "    '''\n",
        "    Args:\n",
        "        - dataframse: a list of `DataFrame` to be joined\n",
        "        - column_name: the column over which the records should be joined\n",
        "        \n",
        "    Returns:\n",
        "        a new dataframes resulting from the join of all the dataframes\n",
        "        over the `column_name` column\n",
        "    '''\n",
        "    joined_df = dataframes[0]\n",
        "\n",
        "    for dataframe in dataframes[1:]:\n",
        "        joined_df = joined_df.join(dataframe, [column_name])\n",
        "\n",
        "    return joined_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Combine Weather Measurements with City Attribute**\n",
        "\n",
        "\n",
        "This code segment iterates over each city in a DataFrame (city_attributes_df), filters several DataFrames containing weather measurements based on the city, and then joins them together while adding city attributes as columns. Finally, it aggregates these DataFrames into a main DataFrame (weather_measurements_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "# Initialize the main DataFrame to store weather measurements\n",
        "weather_measurements_df = None\n",
        "\n",
        "# Collect city attributes as a list\n",
        "city_attributes_list = city_attributes_df.collect()\n",
        "\n",
        "# Iterate over each city and its attributes\n",
        "for row in city_attributes_list:\n",
        "    # Extract attributes for the current city\n",
        "    city = row.City\n",
        "    country = row.Country\n",
        "    latitude = row.Latitude\n",
        "    longitude = row.Longitude\n",
        "\n",
        "    # Filter dataframes for each weather measurement by city\n",
        "    dataframes = [\n",
        "        filter_dataframe_by_city_column(humidity_df, city, HUMIDITY_COL),\n",
        "        filter_dataframe_by_city_column(pressure_df, city, PRESSURE_COL),\n",
        "        filter_dataframe_by_city_column(temperature_df, city, TEMPERATURE_COL),\n",
        "        filter_dataframe_by_city_column(wind_direction_df, city, WIND_DIRECTION_COL),\n",
        "        filter_dataframe_by_city_column(wind_speed_df, city, WIND_SPEED_COL),\n",
        "        filter_dataframe_by_city_column(weather_conditions_df, city, WEATHER_CONDITION_COL)\n",
        "    ]\n",
        "\n",
        "    # Join filtered dataframes based on datetime column and add city attributes as columns\n",
        "    joined_df = join_dataframes(dataframes, DATETIME_COL) \\\n",
        "        .withColumn(CITY_COL, lit(city)) \\\n",
        "        .withColumn(COUNTRY_COL, lit(country)) \\\n",
        "        .withColumn(LATITUDE_COL, lit(latitude)) \\\n",
        "        .withColumn(LONGITUDE_COL, lit(longitude))\n",
        "\n",
        "    # Aggregate the DataFrames computed for each city into the main DataFrame \n",
        "    # by appending them iteratively, ensuring all city measurements are combined.\n",
        "    weather_measurements_df = weather_measurements_df.union(joined_df) if weather_measurements_df is not None else joined_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the shape (number of rows and columns) of the weather_measurements_df DataFrame\n",
        "print_dataframe_shape(weather_measurements_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the schema of the weather_measurements_df DataFrame\n",
        "weather_measurements_df.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if SLOW_OPERATIONS flag is set to True before displaying the first 5 rows of the DataFrame\n",
        "if SLOW_OPERATIONS:\n",
        "    weather_measurements_df.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if SLOW_OPERATIONS flag is set to True before describing the DataFrame using Koalas\n",
        "if SLOW_OPERATIONS:\n",
        "    weather_measurements_df.describe().to_koalas().transpose()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if SLOW_OPERATIONS flag is set to True before counting missing values for each column\n",
        "if SLOW_OPERATIONS:\n",
        "    for c in weather_measurements_df.columns:\n",
        "        print(f'Missing values of column `{c}` count: {weather_measurements_df.where(col(c).isNull()).count()}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a new DataFrame excluding rows with null values\n",
        "not_null_weather_measurements_df = weather_measurements_df.dropna()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if SLOW_OPERATIONS flag is set to True before grouping by weather conditions and counting occurrences\n",
        "if SLOW_OPERATIONS:\n",
        "    not_null_weather_measurements_df.groupBy(WEATHER_CONDITION_COL).count().show(truncate=False)\n"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "mostRecentlyExecutedCommandWithImplicitDF": {
          "commandId": 1680046112603821,
          "dataframes": [
            "_sqldf"
          ]
        },
        "pythonIndentUnit": 4
      },
      "notebookName": "Untitled Notebook 2024-03-19 15:30:36",
      "widgets": {}
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
