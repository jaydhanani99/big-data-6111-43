{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f168103-cb1e-4b64-bb97-8751efbc767e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Weather forecasting with PySpark\n",
    "## Big Data Computing final project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f9d64b0-bcaa-48c0-b8a6-05cb6147892c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Define some global constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94b551c2-61ce-4055-bd75-9cf63c685f05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DATASET_PATH: str = 'dbfs:/bigdata_proj/datasets/historical-hourly-weather/'\n",
    "MODELS_PATH: str = 'dbfs:/bigdata_proj/models/historical-hourly-weather/'\n",
    "\n",
    "RANDOM_SEED: int = 42\n",
    "  \n",
    "SLOW_OPERATIONS: bool = True\n",
    "  \n",
    "# True to save the computation of datataset preprocessing, fitted pipelines and trained models to the filesystem\n",
    "SAVE_COMPUTATIONS: bool = True\n",
    "  \n",
    "# True to load the sampled dataset from the filesystem, False to compute it from the raw one\n",
    "LOAD_SAMPLED_DATASET: bool = True\n",
    "SAMPLED_DATASET_PATH: str = f'{DATASET_PATH}aggregated_sampled_weather_measurements.csv'\n",
    "  \n",
    "# True to load the encoding pipeline from the filesystem, False to compute it from scratch\n",
    "LOAD_ECONDING_PIPELINE: bool = True\n",
    "ENCODING_PIPELINE_PATH: str = f'{MODELS_PATH}data_encoder'\n",
    "\n",
    "# True to load pretrained models from the filesystem, False to compute them from scratch\n",
    "LOAD_PRETRAINED_MODELS: bool = True\n",
    "RANDOM_FOREST_MODEL_PATH: str = f'{MODELS_PATH}rnd_forest'\n",
    "RANDOM_FOREST_CROSS_VALIDATION_MODEL_PATH: str = f'{MODELS_PATH}rnd_forest_cv'\n",
    "LOGISTIC_REGRESSION_CROSS_VALIDATION_MODEL_PATH: str = f'{MODELS_PATH}log_reg_cv'\n",
    "\n",
    "# necessary due to DataBricks community edition limits (training on a dataframe larger than this threshold causes an Internal Server Error)\n",
    "MAX_TRAIN_SIZE: int = 999_999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d95064a1-afae-4596-9ce1-3f7c617b2cf8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Import PySpark packages and other dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78bd20fe-eb60-4c47-9bda-ccf358d08ffb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "from pyspark.ml.tuning import CrossValidatorModel\n",
    "\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11b2fecb-a80c-4202-83bd-185690f85764",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Dataset initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8935700-3499-4982-9208-e74a707e690e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Download the dataset\n",
    "Original source: [kaggle.com/selfishgene/historical-hourly-weather-data](https://www.kaggle.com/selfishgene/historical-hourly-weather-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af85f425-ccd3-409a-8884-cf45053a5ab1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-16 17:05:24 URL:https://raw.githubusercontent.com/andrea-gasparini/big-data-weather-forecasting/master/dataset/historical-hourly-weather-dataset.zip [12655281/12655281] -> \"/tmp/dataset.zip\" [1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /tmp/dataset.zip\n",
      "   creating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/\n",
      "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/._committed_7616641238230246128.crc  \n",
      "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/.part-00000-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5515-1-c000.csv.crc  \n",
      "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/.part-00006-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5521-1-c000.csv.crc  \n",
      "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/.part-00002-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5517-1-c000.csv.crc  \n",
      "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/.part-00004-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5519-1-c000.csv.crc  \n",
      "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/part-00003-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5518-1-c000.csv  \n",
      "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/_committed_7616641238230246128  \n",
      "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/_committed_vacuum6920010656788597249  \n",
      "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/part-00005-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5520-1-c000.csv  \n",
      "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/part-00002-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5517-1-c000.csv  \n",
      "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/_started_5785058191842647654  \n",
      "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/part-00001-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5516-1-c000.csv  \n",
      "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/_committed_777980178201020908  \n",
      "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/_committed_5785058191842647654  \n",
      "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/._committed_5785058191842647654.crc  \n",
      "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/.part-00001-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5516-1-c000.csv.crc  \n",
      "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/.part-00003-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5518-1-c000.csv.crc  \n",
      "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/part-00007-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5522-1-c000.csv  \n",
      "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/part-00006-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5521-1-c000.csv  \n",
      "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/._started_5785058191842647654.crc  \n",
      "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/._committed_777980178201020908.crc  \n",
      "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/part-00000-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5515-1-c000.csv  \n",
      "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/.part-00007-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5522-1-c000.csv.crc  \n",
      "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/.part-00005-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5520-1-c000.csv.crc  \n",
      "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/part-00004-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5519-1-c000.csv  \n",
      "  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/._committed_vacuum6920010656788597249.crc  \n",
      "  inflating: /tmp/dataset/city_attributes.csv  \n",
      "  inflating: /tmp/dataset/humidity.csv  \n",
      "  inflating: /tmp/dataset/pressure.csv  \n",
      "  inflating: /tmp/dataset/temperature.csv  \n",
      "  inflating: /tmp/dataset/weather_description.csv  \n",
      "  inflating: /tmp/dataset/wind_direction.csv  \n",
      "  inflating: /tmp/dataset/wind_speed.csv  \n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "wget --no-verbose https://github.com/andrea-gasparini/big-data-weather-forecasting/raw/master/dataset/historical-hourly-weather-dataset.zip -O /tmp/dataset.zip\n",
    "unzip -u /tmp/dataset.zip -d /tmp/dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54686efa-a033-4197-a7e0-9559fe961d48",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Move the dataset from Databricks local driver node's file system to DBFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cce2f5a-7a72-4e19-945d-78d45f664546",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for file in dbutils.fs.ls('file:/tmp/dataset'):\n",
    "    dbutils.fs.mv(file.path, f'{DATASET_PATH}{file.name}', recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84a9ccdc-b123-4083-85ce-c679b186b645",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/bigdata_proj/datasets/historical-hourly-weather/aggregated_sampled_weather_measurements.csv/</td><td>aggregated_sampled_weather_measurements.csv/</td><td>0</td><td>1710607244000</td></tr><tr><td>dbfs:/bigdata_proj/datasets/historical-hourly-weather/city_attributes.csv</td><td>city_attributes.csv</td><td>1614</td><td>1710608726000</td></tr><tr><td>dbfs:/bigdata_proj/datasets/historical-hourly-weather/humidity.csv</td><td>humidity.csv</td><td>9075077</td><td>1710608725000</td></tr><tr><td>dbfs:/bigdata_proj/datasets/historical-hourly-weather/pressure.csv</td><td>pressure.csv</td><td>12155911</td><td>1710608726000</td></tr><tr><td>dbfs:/bigdata_proj/datasets/historical-hourly-weather/temperature.csv</td><td>temperature.csv</td><td>13971171</td><td>1710608726000</td></tr><tr><td>dbfs:/bigdata_proj/datasets/historical-hourly-weather/weather_description.csv</td><td>weather_description.csv</td><td>21858089</td><td>1710608728000</td></tr><tr><td>dbfs:/bigdata_proj/datasets/historical-hourly-weather/wind_direction.csv</td><td>wind_direction.csv</td><td>10171003</td><td>1710608726000</td></tr><tr><td>dbfs:/bigdata_proj/datasets/historical-hourly-weather/wind_speed.csv</td><td>wind_speed.csv</td><td>7457531</td><td>1710608728000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/bigdata_proj/datasets/historical-hourly-weather/aggregated_sampled_weather_measurements.csv/",
         "aggregated_sampled_weather_measurements.csv/",
         0,
         1710607244000
        ],
        [
         "dbfs:/bigdata_proj/datasets/historical-hourly-weather/city_attributes.csv",
         "city_attributes.csv",
         1614,
         1710878212000
        ],
        [
         "dbfs:/bigdata_proj/datasets/historical-hourly-weather/humidity.csv",
         "humidity.csv",
         9075077,
         1710878209000
        ],
        [
         "dbfs:/bigdata_proj/datasets/historical-hourly-weather/pressure.csv",
         "pressure.csv",
         12155911,
         1710878209000
        ],
        [
         "dbfs:/bigdata_proj/datasets/historical-hourly-weather/temperature.csv",
         "temperature.csv",
         13971171,
         1710878210000
        ],
        [
         "dbfs:/bigdata_proj/datasets/historical-hourly-weather/weather_description.csv",
         "weather_description.csv",
         21858089,
         1710878208000
        ],
        [
         "dbfs:/bigdata_proj/datasets/historical-hourly-weather/wind_direction.csv",
         "wind_direction.csv",
         10171003,
         1710878208000
        ],
        [
         "dbfs:/bigdata_proj/datasets/historical-hourly-weather/wind_speed.csv",
         "wind_speed.csv",
         7457531,
         1710878212000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls /bigdata_proj/datasets/historical-hourly-weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2b0f8a5-d709-4dfa-b734-f4952f5452f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Load dataset into Spark DataFrame objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ce1bbf6-28aa-4199-904b-ebb5d4f236c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Dataset shape and schema\n",
    "The raw dataset downloaded from kaggle is composed of 7 different `.csv` files:\n",
    "- `city_attributes.csv` contains geographical information about the different cities for which there are weather measurements\n",
    "- `weather_description.csv` contains the textual description of the weather conditions, where each column refers to a different city and each row refers to a specific `datetime` in which the weather condition occurred\n",
    "- Each one of the other 5 csv follows the same structure as `weather_description.csv` and contains the measurements of the following metrics: `humidity`,  `pressure`, `temperature`, `wind_direction`, `wind_speed`\n",
    "\n",
    "Except for `city_attributes.csv`, all the other files contains about **45.000** records of hourly weather measurements, that multiplied by the **36** cities results in approximately **1.500.000** records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18a1bfb4-dc11-4ab2-9f4c-2a6ad73046d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# rest of your code\n",
    "weather_conditions_df = spark.read.csv(f'{DATASET_PATH}weather_description.csv', header=True, inferSchema=True)\n",
    "humidity_df = spark.read.csv(f'{DATASET_PATH}humidity.csv', header=True, inferSchema=True)\n",
    "pressure_df = spark.read.csv(f'{DATASET_PATH}pressure.csv', header=True, inferSchema=True)\n",
    "temperature_df = spark.read.csv(f'{DATASET_PATH}temperature.csv', header=True, inferSchema=True)\n",
    "city_attributes_df = spark.read.csv(f'{DATASET_PATH}city_attributes.csv', header=True, inferSchema=True)\n",
    "wind_direction_df = spark.read.csv(f'{DATASET_PATH}wind_direction.csv', header=True, inferSchema=True)\n",
    "wind_speed_df = spark.read.csv(f'{DATASET_PATH}wind_speed.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trying to print the shape and see whats going on**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27698c44-aac3-45ec-b5f3-177437a45cda",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def print_dataframe_shape(dataframe: DataFrame) -> None:\n",
    "    rows_count = dataframe.count()\n",
    "    columns_count = len(dataframe.columns)\n",
    "    print(f'The shape of the dataset is {rows_count} rows by {columns_count} columns', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the function above for city_attributes.csv, Additonally pringing the shema as well**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "989fa830-747b-424c-825c-dc9793ee4a66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the dataset is 36 rows by 4 columns\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_dataframe_shape(city_attributes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b637afb-4cdf-4755-b1ee-1cc43ba4b38a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "city_attributes_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b10ed8a-5eaf-42d1-b87b-37641f8df6b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(City='Vancouver', Country='Canada', Latitude=49.24966, Longitude=-123.119339),\n",
       " Row(City='Portland', Country='United States', Latitude=45.523449, Longitude=-122.676208),\n",
       " Row(City='San Francisco', Country='United States', Latitude=37.774929, Longitude=-122.419418),\n",
       " Row(City='Seattle', Country='United States', Latitude=47.606209, Longitude=-122.332069),\n",
       " Row(City='Los Angeles', Country='United States', Latitude=34.052231, Longitude=-118.243683)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_attributes_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the function print_data_frame we made above for weather_description.csv, Additonally pringing the shema as well.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a746592-efcf-4f76-adbb-3b4028e69d3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the dataset is 45253 rows by 37 columns\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_dataframe_shape(weather_conditions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3924105-4978-4055-a18d-80168f4c183d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datetime: timestamp (nullable = true)\n",
      " |-- Vancouver: string (nullable = true)\n",
      " |-- Portland: string (nullable = true)\n",
      " |-- San Francisco: string (nullable = true)\n",
      " |-- Seattle: string (nullable = true)\n",
      " |-- Los Angeles: string (nullable = true)\n",
      " |-- San Diego: string (nullable = true)\n",
      " |-- Las Vegas: string (nullable = true)\n",
      " |-- Phoenix: string (nullable = true)\n",
      " |-- Albuquerque: string (nullable = true)\n",
      " |-- Denver: string (nullable = true)\n",
      " |-- San Antonio: string (nullable = true)\n",
      " |-- Dallas: string (nullable = true)\n",
      " |-- Houston: string (nullable = true)\n",
      " |-- Kansas City: string (nullable = true)\n",
      " |-- Minneapolis: string (nullable = true)\n",
      " |-- Saint Louis: string (nullable = true)\n",
      " |-- Chicago: string (nullable = true)\n",
      " |-- Nashville: string (nullable = true)\n",
      " |-- Indianapolis: string (nullable = true)\n",
      " |-- Atlanta: string (nullable = true)\n",
      " |-- Detroit: string (nullable = true)\n",
      " |-- Jacksonville: string (nullable = true)\n",
      " |-- Charlotte: string (nullable = true)\n",
      " |-- Miami: string (nullable = true)\n",
      " |-- Pittsburgh: string (nullable = true)\n",
      " |-- Toronto: string (nullable = true)\n",
      " |-- Philadelphia: string (nullable = true)\n",
      " |-- New York: string (nullable = true)\n",
      " |-- Montreal: string (nullable = true)\n",
      " |-- Boston: string (nullable = true)\n",
      " |-- Beersheba: string (nullable = true)\n",
      " |-- Tel Aviv District: string (nullable = true)\n",
      " |-- Eilat: string (nullable = true)\n",
      " |-- Haifa: string (nullable = true)\n",
      " |-- Nahariyya: string (nullable = true)\n",
      " |-- Jerusalem: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_conditions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1444b98-ab35-4d1b-a01d-7a3dd7231508",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(datetime=datetime.datetime(2012, 10, 1, 12, 0), Vancouver=None, Portland=None, San Francisco=None, Seattle=None),\n",
       " Row(datetime=datetime.datetime(2012, 10, 1, 13, 0), Vancouver='mist', Portland='scattered clouds', San Francisco='light rain', Seattle='sky is clear'),\n",
       " Row(datetime=datetime.datetime(2012, 10, 1, 14, 0), Vancouver='broken clouds', Portland='scattered clouds', San Francisco='sky is clear', Seattle='sky is clear'),\n",
       " Row(datetime=datetime.datetime(2012, 10, 1, 15, 0), Vancouver='broken clouds', Portland='scattered clouds', San Francisco='sky is clear', Seattle='sky is clear'),\n",
       " Row(datetime=datetime.datetime(2012, 10, 1, 16, 0), Vancouver='broken clouds', Portland='scattered clouds', San Francisco='sky is clear', Seattle='sky is clear')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_conditions_df[weather_conditions_df.columns[:5]].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preprocessing Starts\n",
    "\n",
    "### For Columns\n",
    "Setting some alias for colums to standardize throughout the code further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b5875bd-28ab-4b84-a4af-11f4af6a9b55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DATETIME_COL = 'datetime'\n",
    "HUMIDITY_COL = 'humidity'\n",
    "PRESSURE_COL = 'pressure'\n",
    "TEMPERATURE_COL = 'temperature'\n",
    "WIND_DIRECTION_COL = 'wind_direction'\n",
    "WIND_SPEED_COL = 'wind_speed'\n",
    "LATITUDE_COL = 'latitude'\n",
    "LONGITUDE_COL = 'longitude'\n",
    "CITY_COL = 'city'\n",
    "COUNTRY_COL = 'country'\n",
    "WEATHER_CONDITION_COL = 'weather_condition'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a single DataFrame that includes all data from the others**\n",
    "\n",
    "Reason to do this : The as we have seen from the shapes above, the dataset needs to be made suitable for Machine Learning purposes. The bese solution here is to use a single DataFrame object which includes all the information. Here we can have one column each for each metric, maybe some columns for the information such as city, geographical positioning etc and lastely one column for weather condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f75a3a92-7975-4f28-b9e7-4e18af003e88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def filter_dataframe_by_city_column(dataframe: DataFrame,\n",
    "                                    city_name: str,\n",
    "                                    new_column_name: str) -> DataFrame:\n",
    "    '''\n",
    "    Args:\n",
    "        - dataframe: a `DataFrame` with a datetime column and n cities columns,\n",
    "                     where the records are the related hourly measurements\n",
    "        - city_name: city name between the ones in the dataframe\n",
    "        - new_column_name: name to replace the city name\n",
    "        \n",
    "    Returns: \n",
    "        a new `DataFrame` with:\n",
    "            - the datetime column\n",
    "            - a single column of measurements related to the `city_name`\n",
    "              and renamed as `new_column_name`\n",
    "    '''\n",
    "    return dataframe.withColumn(new_column_name, col(city_name)) \\\n",
    "                    .select([DATETIME_COL, new_column_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**performing the join operation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7040473b-6615-4434-853e-af0983e66a39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def join_dataframes(dataframes: List[DataFrame], column_name: str) -> DataFrame:\n",
    "    '''\n",
    "    Args:\n",
    "        - dataframse: a list of `DataFrame` to be joined\n",
    "        - column_name: the column over which the records should be joined\n",
    "        \n",
    "    Returns:\n",
    "        a new dataframes resulting from the join of all the dataframes\n",
    "        over the `column_name` column\n",
    "    '''\n",
    "    joined_df = dataframes[0]\n",
    "\n",
    "    for dataframe in dataframes[1:]:\n",
    "        joined_df = joined_df.join(dataframe, [column_name])\n",
    "\n",
    "    return joined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine Weather Measurements with City Attribute**\n",
    "\n",
    "This code segment iterates over each city in a DataFrame (city_attributes_df), filters several DataFrames containing weather measurements based on the city, and then joins them together while adding city attributes as columns. Finally, it aggregates these DataFrames into a main DataFrame (weather_measurements_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "171c8db9-779c-4fbc-8936-a3b41b38e50a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Initialize the main DataFrame to store weather measurements\n",
    "weather_measurements_df = None\n",
    "\n",
    "# Collect city attributes as a list\n",
    "city_attributes_list = city_attributes_df.collect()\n",
    "\n",
    "# Iterate over each city and its attributes\n",
    "for row in city_attributes_list:\n",
    "    # Extract attributes for the current city\n",
    "    city = row.City\n",
    "    country = row.Country\n",
    "    latitude = row.Latitude\n",
    "    longitude = row.Longitude\n",
    "\n",
    "    # Filter dataframes for each weather measurement by city\n",
    "    dataframes = [\n",
    "        filter_dataframe_by_city_column(humidity_df, city, HUMIDITY_COL),\n",
    "        filter_dataframe_by_city_column(pressure_df, city, PRESSURE_COL),\n",
    "        filter_dataframe_by_city_column(temperature_df, city, TEMPERATURE_COL),\n",
    "        filter_dataframe_by_city_column(wind_direction_df, city, WIND_DIRECTION_COL),\n",
    "        filter_dataframe_by_city_column(wind_speed_df, city, WIND_SPEED_COL),\n",
    "        filter_dataframe_by_city_column(weather_conditions_df, city, WEATHER_CONDITION_COL)\n",
    "    ]\n",
    "\n",
    "    # Join filtered dataframes based on datetime column and add city attributes as columns\n",
    "    joined_df = join_dataframes(dataframes, DATETIME_COL) \\\n",
    "        .withColumn(CITY_COL, lit(city)) \\\n",
    "        .withColumn(COUNTRY_COL, lit(country)) \\\n",
    "        .withColumn(LATITUDE_COL, lit(latitude)) \\\n",
    "        .withColumn(LONGITUDE_COL, lit(longitude))\n",
    "\n",
    "    # Aggregate the DataFrames computed for each city into the main DataFrame \n",
    "    # by appending them iteratively, ensuring all city measurements are combined.\n",
    "    weather_measurements_df = weather_measurements_df.union(joined_df) if weather_measurements_df is not None else joined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bda4a338-27c4-4525-a343-0b0b42a117cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the dataset is 1629108 rows by 11 columns\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the shape (number of rows and columns) of the weather_measurements_df DataFrame\n",
    "print_dataframe_shape(weather_measurements_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0744c120-b76c-4e8f-851b-bb88b984fb22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datetime: timestamp (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- wind_direction: double (nullable = true)\n",
      " |-- wind_speed: double (nullable = true)\n",
      " |-- weather_condition: string (nullable = true)\n",
      " |-- city: string (nullable = false)\n",
      " |-- country: string (nullable = false)\n",
      " |-- latitude: double (nullable = false)\n",
      " |-- longitude: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema of the weather_measurements_df DataFrame\n",
    "weather_measurements_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aa1fcd6-4ce4-4c05-afe3-599ca3ac9d3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+--------+-------------+--------------+----------+-----------------+---------+-------+--------+-----------+\n",
      "|           datetime|humidity|pressure|  temperature|wind_direction|wind_speed|weather_condition|     city|country|latitude|  longitude|\n",
      "+-------------------+--------+--------+-------------+--------------+----------+-----------------+---------+-------+--------+-----------+\n",
      "|2012-10-01 12:00:00|    NULL|    NULL|         NULL|          NULL|      NULL|             NULL|Vancouver| Canada|49.24966|-123.119339|\n",
      "|2012-10-01 13:00:00|    76.0|    NULL|       284.63|           0.0|       0.0|             mist|Vancouver| Canada|49.24966|-123.119339|\n",
      "|2012-10-01 14:00:00|    76.0|    NULL| 284.62904131|           6.0|       0.0|    broken clouds|Vancouver| Canada|49.24966|-123.119339|\n",
      "|2012-10-01 15:00:00|    76.0|    NULL|284.626997923|          20.0|       0.0|    broken clouds|Vancouver| Canada|49.24966|-123.119339|\n",
      "|2012-10-01 16:00:00|    77.0|    NULL|284.624954535|          34.0|       0.0|    broken clouds|Vancouver| Canada|49.24966|-123.119339|\n",
      "+-------------------+--------+--------+-------------+--------------+----------+-----------------+---------+-------+--------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if SLOW_OPERATIONS flag is set to True before displaying the first 5 rows of the DataFrame\n",
    "if SLOW_OPERATIONS:\n",
    "    weather_measurements_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a908d798-bf6a-46af-a7b0-57e179feeef2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if SLOW_OPERATIONS flag is set to True before describing the DataFrame using Koalas\n",
    "if SLOW_OPERATIONS:\n",
    "    weather_measurements_df.describe().to_koalas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a35638d4-5c78-4fc9-8a40-1323a4047ed0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values of column `datetime` count: 0\n",
      "Missing values of column `humidity` count: 28651\n",
      "Missing values of column `pressure` count: 16680\n",
      "Missing values of column `temperature` count: 8030\n",
      "Missing values of column `wind_direction` count: 7975\n",
      "Missing values of column `wind_speed` count: 7993\n",
      "Missing values of column `weather_condition` count: 7955\n",
      "Missing values of column `city` count: 0\n",
      "Missing values of column `country` count: 0\n",
      "Missing values of column `latitude` count: 0\n",
      "Missing values of column `longitude` count: 0\n"
     ]
    }
   ],
   "source": [
    "# Check if SLOW_OPERATIONS flag is set to True before counting missing values for each column\n",
    "if SLOW_OPERATIONS:\n",
    "    for c in weather_measurements_df.columns:\n",
    "        print(f'Missing values of column `{c}` count: {weather_measurements_df.where(col(c).isNull()).count()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "714e3edd-96f9-4d43-bed3-d7b173714903",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a new DataFrame excluding rows with null values\n",
    "not_null_weather_measurements_df = weather_measurements_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85a3f9e4-5691-4d5d-9417-8057070b727b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------+\n",
      "|weather_condition           |count |\n",
      "+----------------------------+------+\n",
      "|fog                         |16185 |\n",
      "|very heavy rain             |1001  |\n",
      "|proximity shower rain       |2339  |\n",
      "|few clouds                  |133685|\n",
      "|heavy shower snow           |336   |\n",
      "|light rain                  |127364|\n",
      "|light intensity drizzle     |8048  |\n",
      "|light intensity shower rain |3633  |\n",
      "|broken clouds               |167102|\n",
      "|overcast clouds             |133778|\n",
      "|light snow                  |14368 |\n",
      "|scattered clouds            |143277|\n",
      "|thunderstorm with heavy rain|396   |\n",
      "|thunderstorm with light rain|1179  |\n",
      "|heavy intensity rain        |14075 |\n",
      "|moderate rain               |43172 |\n",
      "|light intensity drizzle rain|41    |\n",
      "|sky is clear                |641577|\n",
      "|snow                        |3156  |\n",
      "|light shower snow           |998   |\n",
      "+----------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if SLOW_OPERATIONS flag is set to True before grouping by weather conditions and counting occurrences\n",
    "if SLOW_OPERATIONS:\n",
    "    not_null_weather_measurements_df.groupBy(WEATHER_CONDITION_COL).count().show(truncate=False)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorizing Weather Conditions**\n",
    "\n",
    "This function categorizes a collection of string weather conditions into broader categories such as thunderstorm, rainy, snowy, cloudy, foggy, or sunny. It iterates through each weather condition, converting them to lowercase for case-insensitive matching. Based on keywords present in each condition, it assigns them to one of the predefined categories. Finally, it returns a dictionary mapping each original weather condition to its corresponding category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23c36250-c44a-4c0e-b1c1-d3dabd66a09b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_weather_conditions_aggregation_dict(weather_conditions: Iterable[str]) -> Dict[str, str]:\n",
    "    '''\n",
    "    Args:\n",
    "        - weather_conditions: an iterable collection of string weather conditions to be aggregated\n",
    "\n",
    "    Returns:\n",
    "        a dictionary that maps from the original weather condition name to one of the following categories:\n",
    "            - thunderstorm\n",
    "            - rainy\n",
    "            - snowy\n",
    "            - cloudy\n",
    "            - foggy\n",
    "            - sunny\n",
    "    '''\n",
    "    \n",
    "    # Initialize an empty dictionary to store aggregated weather conditions\n",
    "    weather_conditions_dict = dict()\n",
    "  \n",
    "    # Iterate over each weather condition\n",
    "    for weather_condition in weather_conditions:\n",
    "  \n",
    "        # Convert weather condition to lowercase for case-insensitive matching\n",
    "        weather_condition_lowered = weather_condition.lower()\n",
    "\n",
    "        # Check for keywords in weather condition to assign category\n",
    "        if any(key in weather_condition_lowered for key in ['squall', 'thunderstorm']):\n",
    "            weather_conditions_dict[weather_condition] = 'thunderstorm'\n",
    "        elif any(key in weather_condition_lowered for key in ['drizzle', 'rain']):\n",
    "            weather_conditions_dict[weather_condition] = 'rainy'\n",
    "        elif any(key in weather_condition_lowered for key in ['sleet', 'snow']):\n",
    "            weather_conditions_dict[weather_condition] = 'snowy'\n",
    "        elif 'cloud' in weather_condition_lowered:\n",
    "            weather_conditions_dict[weather_condition] = 'cloudy'\n",
    "        elif any(key in weather_condition_lowered for key in ['fog', 'mist', 'haze']):\n",
    "            weather_conditions_dict[weather_condition] = 'foggy'\n",
    "        elif any(key in weather_condition_lowered for key in ['clear', 'sun']):\n",
    "            weather_conditions_dict[weather_condition] = 'sunny'\n",
    "            \n",
    "    return weather_conditions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "068c5299-45ac-4bd6-b76a-d3435926df91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weather_conditions_all = not_null_weather_measurements_df \\\n",
    "    .select(WEATHER_CONDITION_COL).distinct() \\\n",
    "    .to_koalas().to_numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0780d1dd-d621-45c1-91e5-759c8d16098e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weather_conditions_dict = get_weather_conditions_aggregation_dict(weather_conditions_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afd57a15-357c-491a-83be-184afc4dd9c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weather_measurements_aggregated_df = not_null_weather_measurements_df.replace(weather_conditions_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a9796a3-6fe9-47cb-ba2e-cef974e542a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "WEATHER_CONDITIONS = set(weather_conditions_dict.values())\n",
    "\n",
    "weather_measurements_aggregated_df = weather_measurements_aggregated_df \\\n",
    "    .filter(weather_measurements_aggregated_df[WEATHER_CONDITION_COL].isin(WEATHER_CONDITIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "524ca0ce-2029-4932-b25b-ef7a4553e25d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+\n",
      "|weather_condition| count|\n",
      "+-----------------+------+\n",
      "|            rainy|202725|\n",
      "|            snowy| 21283|\n",
      "|            sunny|641577|\n",
      "|           cloudy|577842|\n",
      "|     thunderstorm| 10852|\n",
      "|            foggy|138707|\n",
      "+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if SLOW_OPERATIONS: weather_measurements_aggregated_df.groupBy(WEATHER_CONDITION_COL).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "645e38f2-546e-4e4f-80b0-3b7ba7b7ea4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def count_weather_condition_occurrences(dataframe: DataFrame, class_name: str) -> int:\n",
    "    '''\n",
    "    Args:\n",
    "        - dataframe: a `DataFrame` which contains a column `WEATHER_CONDITION_COL`\n",
    "        - class_name: the class name to count the occurences of\n",
    "        \n",
    "    Returns:\n",
    "        the total number of `class_name` occurences inside `dataframe`\n",
    "    '''\n",
    "    return dataframe.filter(dataframe[WEATHER_CONDITION_COL] == class_name).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cf48268-4cb7-4226-bf95-10c6bfd370df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_undersampling_fracs(dataframe: DataFrame) -> Dict[str, float]:\n",
    "    '''\n",
    "    Args:\n",
    "        - dataframe: a `DataFrame` of weather measurements which contains a column `WEATHER_CONDITION_COL`\n",
    "        \n",
    "    Returns:\n",
    "        a dictionary that goes from a weather condition to its fraction\n",
    "        that should be sampled in order to match the occurrences of the minority class\n",
    "    '''\n",
    "\n",
    "    rainy_cnt = count_weather_condition_occurrences(dataframe, 'rainy')\n",
    "    snowy_cnt = count_weather_condition_occurrences(dataframe, 'snowy')\n",
    "    sunny_cnt = count_weather_condition_occurrences(dataframe, 'sunny')\n",
    "    foggy_cnt = count_weather_condition_occurrences(dataframe, 'foggy')\n",
    "    cloudy_cnt = count_weather_condition_occurrences(dataframe, 'cloudy')\n",
    "    thunderstorm_cnt = count_weather_condition_occurrences(dataframe, 'thunderstorm')\n",
    "\n",
    "    minority_class_cnt = np.min(\n",
    "        [rainy_cnt, snowy_cnt, sunny_cnt, cloudy_cnt, foggy_cnt, thunderstorm_cnt]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'rainy': minority_class_cnt / rainy_cnt if rainy_cnt != 0 else 0,\n",
    "        'snowy': minority_class_cnt / snowy_cnt if snowy_cnt != 0 else 0,\n",
    "        'sunny': minority_class_cnt / sunny_cnt if sunny_cnt != 0 else 0,\n",
    "        'foggy': minority_class_cnt / foggy_cnt if foggy_cnt != 0 else 0,\n",
    "        'cloudy': minority_class_cnt / cloudy_cnt if cloudy_cnt != 0 else 0,\n",
    "        'thunderstorm': minority_class_cnt / thunderstorm_cnt if thunderstorm_cnt != 0 else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a10ae9a-a4f2-42cd-b998-8bd5c0d266bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sampled_weather_measurements_df = not_null_weather_measurements_df.sampleBy(WEATHER_CONDITION_COL,\n",
    "                                                                           fractions=get_undersampling_fracs(not_null_weather_measurements_df),\n",
    "                                                                           seed=RANDOM_SEED)\n",
    "if LOAD_SAMPLED_DATASET:\n",
    "    sampled_weather_measurements_df = spark.read.csv(SAMPLED_DATASET_PATH, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b0e02b4-d831-42d1-b9af-9e2298a1a9ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if SAVE_COMPUTATIONS and not LOAD_SAMPLED_DATASET:\n",
    "    sampled_weather_measurements_df.write.csv(SAMPLED_DATASET_PATH,\n",
    "                                              mode='overwrite',\n",
    "                                              header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bb8a8e9-8cde-4f6c-8383-67465bb0f36c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|weather_condition|count|\n",
      "+-----------------+-----+\n",
      "|            rainy| 8373|\n",
      "|            snowy| 8622|\n",
      "|            sunny| 8656|\n",
      "|           cloudy| 8644|\n",
      "|     thunderstorm| 8553|\n",
      "|            foggy| 8581|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampled_weather_measurements_df.groupBy(WEATHER_CONDITION_COL).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aca3016a-fefb-4b9c-b420-f4a1db280078",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>count</td>\n",
       "      <td>mean</td>\n",
       "      <td>stddev</td>\n",
       "      <td>min</td>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>humidity</th>\n",
       "      <td>51429</td>\n",
       "      <td>73.35688035933033</td>\n",
       "      <td>20.633996204510503</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pressure</th>\n",
       "      <td>51429</td>\n",
       "      <td>1017.105601897762</td>\n",
       "      <td>13.325849842166518</td>\n",
       "      <td>803.0</td>\n",
       "      <td>1100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temperature</th>\n",
       "      <td>51429</td>\n",
       "      <td>286.6298669477111</td>\n",
       "      <td>12.068790803267712</td>\n",
       "      <td>243.62</td>\n",
       "      <td>317.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wind_direction</th>\n",
       "      <td>51429</td>\n",
       "      <td>183.87932878337125</td>\n",
       "      <td>103.6648132387239</td>\n",
       "      <td>0.0</td>\n",
       "      <td>360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wind_speed</th>\n",
       "      <td>51429</td>\n",
       "      <td>3.1757957572575783</td>\n",
       "      <td>2.291689346112425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weather_condition</th>\n",
       "      <td>51429</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>thunderstorm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>city</th>\n",
       "      <td>51429</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Albuquerque</td>\n",
       "      <td>Vancouver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <td>51429</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Canada</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>latitude</th>\n",
       "      <td>51429</td>\n",
       "      <td>37.886835537420296</td>\n",
       "      <td>5.830630652854522</td>\n",
       "      <td>25.774269</td>\n",
       "      <td>49.24966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>longitude</th>\n",
       "      <td>51429</td>\n",
       "      <td>-82.26688291139277</td>\n",
       "      <td>38.71590125404249</td>\n",
       "      <td>-123.119339</td>\n",
       "      <td>35.216331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0                   1                   2            3              4\n",
       "summary            count                mean              stddev          min            max\n",
       "humidity           51429   73.35688035933033  20.633996204510503          5.0          100.0\n",
       "pressure           51429   1017.105601897762  13.325849842166518        803.0         1100.0\n",
       "temperature        51429   286.6298669477111  12.068790803267712       243.62         317.35\n",
       "wind_direction     51429  183.87932878337125   103.6648132387239          0.0          360.0\n",
       "wind_speed         51429  3.1757957572575783   2.291689346112425          0.0           35.0\n",
       "weather_condition  51429                None                None       cloudy   thunderstorm\n",
       "city               51429                None                None  Albuquerque      Vancouver\n",
       "country            51429                None                None       Canada  United States\n",
       "latitude           51429  37.886835537420296   5.830630652854522    25.774269       49.24966\n",
       "longitude          51429  -82.26688291139277   38.71590125404249  -123.119339      35.216331"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_weather_measurements_df.describe().to_koalas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cb40317-fbde-411f-94b5-fe4019f8bdc1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = sampled_weather_measurements_df.randomSplit([0.8, 0.2], seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "686b89fd-009b-43e1-a6d2-3fa5fd613c2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# necessary due to DataBricks limits (training on a dataframe larger than this threshold causes an Internal Server Error)\n",
    "train_df = train_df.limit(MAX_TRAIN_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7791ef1-5b62-4b21-95ad-1858cc4001d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size:  41326 instances\n",
      "Test set size:   10103 instances\n"
     ]
    }
   ],
   "source": [
    "print(f'Train set size:  {train_df.count()} instances')\n",
    "print(f'Test set size:   {test_df.count()} instances')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3405706733020154,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bigdata_proj",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
