{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14148323-3459-4ca5-bcd6-b0a8ed1b11fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DATASET_PATH: str = 'dbfs:/bigdata_proj/datasets/historical-hourly-weather/'\n",
    "MODELS_PATH: str = 'dbfs:/bigdata_proj/models/historical-hourly-weather/'\n",
    "RANDOM_SEED: int = 35\n",
    "LOAD_SAMPLED_DATASET = False\n",
    "SAVE_COMPUTATIONS = True\n",
    "SAMPLED_DATASET_PATH: str = f'{DATASET_PATH}aggregated_sampled_weather_measurements.csv'\n",
    "MAX_TRAIN_SIZE: int = 999_999\n",
    "LOAD_ECONDING_PIPELINE: bool = False\n",
    "ENCODING_PIPELINE_PATH: str = f'{MODELS_PATH}data_encoder'\n",
    "LOAD_PRETRAINED_MODELS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "940e0ad6-6533-4971-9c6c-c917f6e9e0f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import mean as _mean\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "from pyspark.ml.tuning import CrossValidatorModel\n",
    "\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b50f1192-d000-45a4-8907-f8c643be9549",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-23 23:02:40 URL:https://raw.githubusercontent.com/jaydhanani99/big-data-6111-43/main/dataset/historical-hourly-weather-dataset.zip [12655281/12655281] -> \"/tmp/dataset.zip\" [1]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /tmp/dataset.zip\n   creating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/\n  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/._committed_7616641238230246128.crc  \n  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/.part-00000-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5515-1-c000.csv.crc  \n  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/.part-00006-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5521-1-c000.csv.crc  \n  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/.part-00002-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5517-1-c000.csv.crc  \n  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/.part-00004-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5519-1-c000.csv.crc  \n  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/part-00003-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5518-1-c000.csv  \n  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/_committed_7616641238230246128  \n  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/_committed_vacuum6920010656788597249  \n  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/part-00005-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5520-1-c000.csv  \n  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/part-00002-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5517-1-c000.csv  \n  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/_started_5785058191842647654  \n  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/part-00001-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5516-1-c000.csv  \n  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/_committed_777980178201020908  \n  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/_committed_5785058191842647654  \n  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/._committed_5785058191842647654.crc  \n  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/.part-00001-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5516-1-c000.csv.crc  \n  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/.part-00003-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5518-1-c000.csv.crc  \n  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/part-00007-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5522-1-c000.csv  \n  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/part-00006-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5521-1-c000.csv  \n  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/._started_5785058191842647654.crc  \n  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/._committed_777980178201020908.crc  \n  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/part-00000-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5515-1-c000.csv  \n  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/.part-00007-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5522-1-c000.csv.crc  \n  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/.part-00005-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5520-1-c000.csv.crc  \n  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/part-00004-tid-5785058191842647654-99694b27-5637-4d82-97fd-79413e3b2b1a-5519-1-c000.csv  \n  inflating: /tmp/dataset/aggregated_sampled_weather_measurements.csv/._committed_vacuum6920010656788597249.crc  \n  inflating: /tmp/dataset/city_attributes.csv  \n  inflating: /tmp/dataset/humidity.csv  \n  inflating: /tmp/dataset/pressure.csv  \n  inflating: /tmp/dataset/temperature.csv  \n  inflating: /tmp/dataset/weather_description.csv  \n  inflating: /tmp/dataset/wind_direction.csv  \n  inflating: /tmp/dataset/wind_speed.csv  \n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "wget --no-verbose https://github.com/jaydhanani99/big-data-6111-43/raw/main/dataset/historical-hourly-weather-dataset.zip -O /tmp/dataset.zip\n",
    "unzip -u /tmp/dataset.zip -d /tmp/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e5b5fbd-1032-4440-9135-4775b8359b5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for file in dbutils.fs.ls('file:/tmp/dataset'):\n",
    "    dbutils.fs.mv(file.path, f'{DATASET_PATH}{file.name}', recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6840ce9-1a80-4667-9487-43c65c6c2575",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# rest of your code\n",
    "weather_conditions_df = spark.read.csv(f'{DATASET_PATH}weather_description.csv', header=True, inferSchema=True)\n",
    "humidity_df = spark.read.csv(f'{DATASET_PATH}humidity.csv', header=True, inferSchema=True)\n",
    "pressure_df = spark.read.csv(f'{DATASET_PATH}pressure.csv', header=True, inferSchema=True)\n",
    "temperature_df = spark.read.csv(f'{DATASET_PATH}temperature.csv', header=True, inferSchema=True)\n",
    "city_attributes_df = spark.read.csv(f'{DATASET_PATH}city_attributes.csv', header=True, inferSchema=True)\n",
    "wind_direction_df = spark.read.csv(f'{DATASET_PATH}wind_direction.csv', header=True, inferSchema=True)\n",
    "wind_speed_df = spark.read.csv(f'{DATASET_PATH}wind_speed.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cd24bfa-ffa0-43c4-adb7-c3b65d2c9cb0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DATETIME_COL = 'datetime'\n",
    "HUMIDITY_COL = 'humidity'\n",
    "PRESSURE_COL = 'pressure'\n",
    "TEMPERATURE_COL = 'temperature'\n",
    "WIND_DIRECTION_COL = 'wind_direction'\n",
    "WIND_SPEED_COL = 'wind_speed'\n",
    "LATITUDE_COL = 'latitude'\n",
    "LONGITUDE_COL = 'longitude'\n",
    "CITY_COL = 'city'\n",
    "COUNTRY_COL = 'country'\n",
    "WEATHER_CONDITION_COL = 'weather_condition'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6184e133-9ef3-430d-b21a-1217d14af5c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def filter_dataframe_by_city_column(dataframe: DataFrame,\n",
    "                                    city_name: str,\n",
    "                                    new_column_name: str) -> DataFrame:\n",
    "    '''\n",
    "    Args:\n",
    "        - dataframe: a `DataFrame` with a datetime column and n cities columns,\n",
    "                     where the records are the related hourly measurements\n",
    "        - city_name: city name between the ones in the dataframe\n",
    "        - new_column_name: name to replace the city name\n",
    "        \n",
    "    Returns: \n",
    "        a new `DataFrame` with:\n",
    "            - the datetime column\n",
    "            - a single column of measurements related to the `city_name`\n",
    "              and renamed as `new_column_name`\n",
    "    '''\n",
    "    return dataframe.withColumn(new_column_name, col(city_name)) \\\n",
    "                    .select([DATETIME_COL, new_column_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa96f7e8-ce91-4451-ba3c-bca4c1a66b8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def join_dataframes(dataframes: List[DataFrame], column_name: str) -> DataFrame:\n",
    "    '''\n",
    "    Args:\n",
    "        - dataframse: a list of `DataFrame` to be joined\n",
    "        - column_name: the column over which the records should be joined\n",
    "        \n",
    "    Returns:\n",
    "        a new dataframes resulting from the join of all the dataframes\n",
    "        over the `column_name` column\n",
    "    '''\n",
    "    joined_df = dataframes[0]\n",
    "\n",
    "    for dataframe in dataframes[1:]:\n",
    "        joined_df = joined_df.join(dataframe, [column_name])\n",
    "\n",
    "    return joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a88f0d77-979b-43db-8641-d42c76402653",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Initialize the main DataFrame to store weather measurements\n",
    "weather_measurements_df = None\n",
    "\n",
    "# Collect city attributes as a list\n",
    "city_attributes_list = city_attributes_df.collect()\n",
    "\n",
    "# Iterate over each city and its attributes\n",
    "for row in city_attributes_list:\n",
    "    # Extract attributes for the current city\n",
    "    city = row.City\n",
    "    country = row.Country\n",
    "    latitude = row.Latitude\n",
    "    longitude = row.Longitude\n",
    "\n",
    "    # Filter dataframes for each weather measurement by city\n",
    "    dataframes = [\n",
    "        filter_dataframe_by_city_column(humidity_df, city, HUMIDITY_COL),\n",
    "        filter_dataframe_by_city_column(pressure_df, city, PRESSURE_COL),\n",
    "        filter_dataframe_by_city_column(temperature_df, city, TEMPERATURE_COL),\n",
    "        filter_dataframe_by_city_column(wind_direction_df, city, WIND_DIRECTION_COL),\n",
    "        filter_dataframe_by_city_column(wind_speed_df, city, WIND_SPEED_COL),\n",
    "        filter_dataframe_by_city_column(weather_conditions_df, city, WEATHER_CONDITION_COL)\n",
    "    ]\n",
    "\n",
    "    # Join filtered dataframes based on datetime column and add city attributes as columns\n",
    "    joined_df = join_dataframes(dataframes, DATETIME_COL) \\\n",
    "        .withColumn(CITY_COL, lit(city)) \\\n",
    "        .withColumn(COUNTRY_COL, lit(country)) \\\n",
    "        .withColumn(LATITUDE_COL, lit(latitude)) \\\n",
    "        .withColumn(LONGITUDE_COL, lit(longitude))\n",
    "\n",
    "    # Aggregate the DataFrames computed for each city into the main DataFrame \n",
    "    # by appending them iteratively, ensuring all city measurements are combined.\n",
    "    weather_measurements_df = weather_measurements_df.union(joined_df) if weather_measurements_df is not None else joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "194a477e-ffd0-4c92-914c-e21dcbe8b1d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "not_null_weather_measurements_df = weather_measurements_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2568f4ca-467e-4c72-8f1a-c8cd5e8a73a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_weather_conditions_aggregation_dict(weather_conditions: Iterable[str]) -> Dict[str, str]:\n",
    "    '''\n",
    "    Args:\n",
    "        - weather_conditions: an iterable collection of string weather conditions to be aggregated\n",
    "\n",
    "    Returns:\n",
    "        a dictionary that maps from the original weather condition name to one of the following categories:\n",
    "            - thunderstorm\n",
    "            - rainy\n",
    "            - snowy\n",
    "            - cloudy\n",
    "            - foggy\n",
    "            - sunny\n",
    "    '''\n",
    "    \n",
    "    # Initialize an empty dictionary to store aggregated weather conditions\n",
    "    weather_conditions_dict = dict()\n",
    "  \n",
    "    # Iterate over each weather condition\n",
    "    for weather_condition in weather_conditions:\n",
    "  \n",
    "        # Convert weather condition to lowercase for case-insensitive matching\n",
    "        weather_condition_lowered = weather_condition.lower()\n",
    "\n",
    "        # Check for keywords in weather condition to assign category\n",
    "        if any(key in weather_condition_lowered for key in ['squall', 'thunderstorm']):\n",
    "            weather_conditions_dict[weather_condition] = 'thunderstorm'\n",
    "        elif any(key in weather_condition_lowered for key in ['drizzle', 'rain']):\n",
    "            weather_conditions_dict[weather_condition] = 'rainy'\n",
    "        elif any(key in weather_condition_lowered for key in ['sleet', 'snow']):\n",
    "            weather_conditions_dict[weather_condition] = 'snowy'\n",
    "        elif 'cloud' in weather_condition_lowered:\n",
    "            weather_conditions_dict[weather_condition] = 'cloudy'\n",
    "        elif any(key in weather_condition_lowered for key in ['clear', 'sun']):\n",
    "            weather_conditions_dict[weather_condition] = 'sunny'\n",
    "            \n",
    "    return weather_conditions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65675979-cca0-41ab-bacf-d4fdf86faf6a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weather_conditions_all = not_null_weather_measurements_df \\\n",
    "    .select(WEATHER_CONDITION_COL) \\\n",
    "    .distinct() \\\n",
    "    .rdd.map(lambda row: row[0]) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06fe9e97-c81b-429f-be11-6435bc921ef1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weather_conditions_dict = get_weather_conditions_aggregation_dict(weather_conditions_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61071870-7eb6-4a2f-bdbb-5daf15e44ace",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weather_measurements_aggregated_df = not_null_weather_measurements_df.replace(weather_conditions_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76089e18-89f3-4ad7-adb2-a83fb8d28c62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "WEATHER_CONDITIONS = set(weather_conditions_dict.values())\n",
    "\n",
    "weather_measurements_aggregated_df = weather_measurements_aggregated_df \\\n",
    "    .filter(weather_measurements_aggregated_df[WEATHER_CONDITION_COL].isin(WEATHER_CONDITIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23948edc-f15b-4c6d-9e14-767fb489eb84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def count_weather_condition_occurrences(dataframe: DataFrame, class_name: str) -> int:\n",
    "    '''\n",
    "    Args:\n",
    "        - dataframe: a `DataFrame` which contains a column `WEATHER_CONDITION_COL`\n",
    "        - class_name: the class name to count the occurences of\n",
    "        \n",
    "    Returns:\n",
    "        the total number of `class_name` occurences inside `dataframe`\n",
    "    '''\n",
    "    return dataframe.filter(dataframe[WEATHER_CONDITION_COL] == class_name).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab1b0483-2eb4-456d-9248-0f53d72ca92e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_undersampling_fracs(dataframe: DataFrame) -> Dict[str, float]:\n",
    "    '''\n",
    "    Args:\n",
    "        - dataframe: a `DataFrame` of weather measurements which contains a column `WEATHER_CONDITION_COL`\n",
    "        \n",
    "    Returns:\n",
    "        a dictionary that goes from a weather condition to its fraction\n",
    "        that should be sampled in order to match the occurrences of the minority class\n",
    "    '''\n",
    "\n",
    "    rainy_cnt = count_weather_condition_occurrences(dataframe, 'rainy')\n",
    "    snowy_cnt = count_weather_condition_occurrences(dataframe, 'snowy')\n",
    "    sunny_cnt = count_weather_condition_occurrences(dataframe, 'sunny')\n",
    "    cloudy_cnt = count_weather_condition_occurrences(dataframe, 'cloudy')\n",
    "    thunderstorm_cnt = count_weather_condition_occurrences(dataframe, 'thunderstorm')\n",
    "\n",
    "    minority_class_cnt = np.min(\n",
    "        [rainy_cnt, snowy_cnt, sunny_cnt, cloudy_cnt, thunderstorm_cnt]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'rainy': minority_class_cnt / rainy_cnt if rainy_cnt != 0 else 0,\n",
    "        'snowy': minority_class_cnt / snowy_cnt if snowy_cnt != 0 else 0,\n",
    "        'sunny': minority_class_cnt / sunny_cnt if sunny_cnt != 0 else 0,\n",
    "        'cloudy': minority_class_cnt / cloudy_cnt if cloudy_cnt != 0 else 0,\n",
    "        'thunderstorm': minority_class_cnt / thunderstorm_cnt if thunderstorm_cnt != 0 else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c561e983-1491-44b9-8030-cf34b174d0a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sampled_weather_measurements_df = weather_measurements_aggregated_df.sampleBy(WEATHER_CONDITION_COL,\n",
    "                                                                           fractions=get_undersampling_fracs(weather_measurements_aggregated_df),\n",
    "                                                                           seed=RANDOM_SEED)\n",
    "if LOAD_SAMPLED_DATASET:\n",
    "    sampled_weather_measurements_df = spark.read.csv(SAMPLED_DATASET_PATH, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55ba6d52-921f-403f-aff6-79ea0a38b47f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if SAVE_COMPUTATIONS and not LOAD_SAMPLED_DATASET:\n",
    "    sampled_weather_measurements_df.write.csv(SAMPLED_DATASET_PATH,\n",
    "                                              mode='overwrite',\n",
    "                                              header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea8e6d69-6816-47b6-b8f3-be9e9ea1ce79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = sampled_weather_measurements_df.randomSplit([0.8, 0.2], seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "553e7357-cc44-414d-8243-a2da16d7e71b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df.limit(MAX_TRAIN_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0bd30c9-eb50-4e02-9099-a30c4a11fa2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size:  43155 instances\nTest set size:   10956 instances\n"
     ]
    }
   ],
   "source": [
    "print(f'Train set size:  {train_df.count()} instances')\n",
    "print(f'Test set size:   {test_df.count()} instances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd4baf4b-0007-42aa-b4bb-19c61fd36f69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "NUMERICAL_FEATURES = [HUMIDITY_COL,\n",
    "                      PRESSURE_COL,\n",
    "                      TEMPERATURE_COL,\n",
    "                      WIND_DIRECTION_COL,\n",
    "                      WIND_SPEED_COL,\n",
    "                      LATITUDE_COL,\n",
    "                      LONGITUDE_COL]\n",
    "\n",
    "CATEGORICAL_FEATURES = []\n",
    "\n",
    "TARGET_VARIABLE_COL = WEATHER_CONDITION_COL\n",
    "PREDICTED_TARGET_VARIABLE_COL = f'predicted_{TARGET_VARIABLE_COL}'\n",
    "\n",
    "LABEL_COL = 'label'\n",
    "PREDICTION_COL = 'prediction'\n",
    "\n",
    "FEATURES_COL = 'features'\n",
    "SCALED_FEATURES_COL = f'scaled_{FEATURES_COL}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7d1b5c6-5f04-430f-8281-c7355d6de6e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def save_pyspark_model(model, path: str, append_datetime: bool = True) -> None:\n",
    "    \n",
    "    if append_datetime:\n",
    "        from datetime import datetime\n",
    "        path += '-' + datetime.now().strftime(\"%Y%d%m-%H%M%S\")\n",
    "    \n",
    "    model.write().overwrite().save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b873387-62c3-4d03-b266-3c351b84fb53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def encoding_pipeline(dataframe: DataFrame,\n",
    "                      numerical_features: List[str],\n",
    "                      categorical_features: List[str],\n",
    "                      target_variable: str,\n",
    "                      with_std: bool = True,\n",
    "                      with_mean: bool = False) -> PipelineModel:\n",
    "    '''\n",
    "    Args:\n",
    "        - dataframe: the input `DataFrame` to fit the pipeline\n",
    "        - numerical_features: the list of column names in `dataframe` corresponding to numerical features\n",
    "        - categorical_features: the list of column names in `dataframe` corresponding to categorical features\n",
    "        - target_variable: the column name in `dataframe` corresponding to the target variable\n",
    "        - with_std: whether to scale the data to unit standard deviation or not (True by default)\n",
    "        - with_mean: whether to center the data with mean before scaling (False by default)\n",
    "\n",
    "    Returns:\n",
    "        the encoding pipeline fitted with `dataframe`\n",
    "    '''\n",
    "    \n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "\n",
    "    # Indexing, i.e. transform to numerical values, the target column and rename it as the variable `LABEL_COL`\n",
    "    label_indexer = StringIndexer(inputCol=target_variable, outputCol=LABEL_COL)\n",
    "    \n",
    "    # Create a list of indexers, one for each categorical feature\n",
    "    indexers = [StringIndexer(inputCol=c, outputCol=f'{c}_indexed', handleInvalid='keep') for c in categorical_features]\n",
    "\n",
    "    # Create the one-hot encoder for the list of features just indexed (this encoder will keep any unseen label in the future)\n",
    "    encoder = OneHotEncoder(inputCols=[indexer.getOutputCol() for indexer in indexers], \n",
    "                            outputCols=[f'{indexer.getOutputCol()}_encoded' for indexer in indexers], \n",
    "                            handleInvalid='keep')\n",
    "    \n",
    "    # Assemble all the features (both one-hot-encoded categorical and numerical) into a single vector\n",
    "    features = encoder.getOutputCols() + numerical_features    \n",
    "    assembler = VectorAssembler(inputCols=features, outputCol=FEATURES_COL)\n",
    "    \n",
    "    # Create a second feature column with the data scaled accordingly to `withStd` and `withMean`\n",
    "    scaler = StandardScaler(inputCol=assembler.getOutputCol(), outputCol=SCALED_FEATURES_COL, withStd=with_std, withMean=with_mean)\n",
    "\n",
    "    stages = [label_indexer] + indexers + [encoder] + [assembler] + [scaler]\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "\n",
    "    transformer = pipeline.fit(dataframe)\n",
    "\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5590de9e-63d4-4d2f-9d3e-77287f4203e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data_encoder = PipelineModel.load(ENCODING_PIPELINE_PATH) if LOAD_ECONDING_PIPELINE \\\n",
    "               else encoding_pipeline(train_df, NUMERICAL_FEATURES, CATEGORICAL_FEATURES, TARGET_VARIABLE_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28fe4665-b6ef-4ff6-af0c-0b752ea5e910",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "encoded_train_df = data_encoder.transform(train_df)\n",
    "encoded_test_df = data_encoder.transform(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0473c50-c9f1-488c-b7bc-233b447352b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n|prediction|label|     scaled_features|\n+----------+-----+--------------------+\n|       1.0|  0.0|[3.65643383571563...|\n|       4.0|  0.0|[3.89698869332850...|\n|       4.0|  4.0|[3.89698869332850...|\n|       4.0|  2.0|[4.4743203515994,...|\n|       2.0|  4.0|[4.4743203515994,...|\n+----------+-----+--------------------+\nonly showing top 5 rows\n\nTest Accuracy = 0.5545819642205184\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Assuming that encoded_train_df and encoded_test_df are already transformed by your encoding_pipeline\n",
    "\n",
    "# Decision Tree model for multiclass classification\n",
    "decision_tree = DecisionTreeClassifier(featuresCol=SCALED_FEATURES_COL, labelCol=LABEL_COL)\n",
    "\n",
    "# Fit the model on the encoded training data\n",
    "dt_model = decision_tree.fit(encoded_train_df)\n",
    "\n",
    "# Make predictions on the encoded test data\n",
    "dt_predictions = dt_model.transform(encoded_test_df)\n",
    "\n",
    "# Show some predictions\n",
    "dt_predictions.select(\"prediction\", LABEL_COL, SCALED_FEATURES_COL).show(5)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=LABEL_COL, predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(dt_predictions)\n",
    "print(f\"Test Accuracy = {accuracy}\")\n",
    "\n",
    "# You can also evaluate other metrics such as F1-score, Precision, Recall by changing the `metricName` parameter in `MulticlassClassificationEvaluator`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "571ff634-0307-42ef-8883-d9bcf18a9d3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5953815261044176\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "# Parameter grid for hyperparameter tuning\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(decision_tree.maxDepth, [5, 10, 15]) \\\n",
    "    .addGrid(decision_tree.maxBins, [32, 40, 50]) \\\n",
    "    .addGrid(decision_tree.minInstancesPerNode, [1, 5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "# Evaluator for cross-validation\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=LABEL_COL, predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Cross-validator\n",
    "crossval = CrossValidator(estimator=decision_tree,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)  # Use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation and choose the best set of parameters\n",
    "cvModel = crossval.fit(encoded_train_df)\n",
    "\n",
    "# Make predictions on test data using the best model found\n",
    "cvPredictions = cvModel.transform(encoded_test_df)\n",
    "\n",
    "# Evaluate the best model's performance on the test data\n",
    "testAccuracy = evaluator.evaluate(cvPredictions)\n",
    "print(f\"Test Accuracy: {testAccuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44280c8f-9611-4a35-913e-3a6b7a579e18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2576668436750275,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "BigData",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
